what is multicolliniarity?
Remember that some level of correlation between variables is normal, but severe multicollinearity can affect the reliability of regression results.
Calculate the correlation coefficients between all pairs of independent variables.
High correlations (close to 1 or -1) indicate potential multicollinearity.
Visualize the correlation matrix using a heatmap for better understanding.
High VIF values (typically above 5 or 10) suggest severe multicollinearity.



what is bias and variance in ridge and lasso?


#LINEAR REGRESSION
IN LINEAR REGRESSION WE TRY FIND BEST FIT LINE WHICH HELP US TO DO PREDICTION.
AFTER CRATING BEST FIT LINE AND IN THAT BEST FIT LINE MY PREDCITED POINTS ON THE BEST FIT LINE SO IF I CALCULATE THE DISTANCE BETWEEN PREDICTED AND ACTUAL POINTS AND THOSE DISTANCE SHOULD BE MINIMUM.IF I DO SUMMATION IT SHOULD BE MINIMAL.

SO THERE'S ANOTHER Way why WE JUST DON'T CREATE multiple lines and try to compare and whoever give best minimal point we'll select it but how many iterations we should strat at one point and finiding best point.

the formula is y=wx+b w(is slope) and b is intercept.
if i change the w it determine how much changes u will see in y when u make some chnange in x so baiscally its responsible for changes in line uper niche.
and b is called intercept because b is the value of y when u said x=0 so that means b is the point at which b intersect with y axis.
and its also called weights and biases.
this equation is our model if u change the this weights and biases it we'll have line that will move around.model will give different predictions when u input certain age it will give different output for charges when we changes the values of w and b.

why we called linear regression?
cuz its alligned and we try to fit the line so we try to make sure the line fits as close as possible to points to actual one.

so if someone said how can we do this code just create two columns age and charges that's it and write the function

def linear(w,x,b):
    return w*x + b


w=50
b=100

r=linear(w,b,30)
now ill have some charges

#REMEMBER IF I WANT TO MOVE LINE DOWN AND UP DEACREASE THE B AND WANT TO UP INCREASE THE B.
IF I WANT TO INCRESE THE SLOPE INCREASE THE W AND TO DECREASE THE SLOP DECREASE W.

SO ALL THIS WE CAN DO THIS WITH OUR EYES WITH HUMAN INTERAACTION BUT HOW MODEL DOING IT ?
IT WOULD BE NICE MODEL CAN DO IT OWN ITS OWN STARTING FROM RANDOM GUESS SO FOR THIS FIND THE COST FUNCTION IT CALCULATE DIFFERENCE BETWEEN PREDICTED VALUE AND ORIGINAL VALUE FOR EVERY DATA POINTS THAT IS CALLED LOSS FUNCTION AND 
IT SQUARE ALL THE ELEMENTS(WHY?CUZ POSITIVE AND NEGATIVE WILLCANCEL OUT )TO REMOVE THE NEGATIVE VALUE AND THEN TAKE AVARAGE OF LOSS VALUES OVER ALL THE INPUT.AND AFTER THAT IT TAKE SQUARE ROOT(WHY? TO CANCEL OUT THE NEGATIVE AND POSITIVE I SQUARED THEM BUT NOW I WANT MY ORIGINAL VALUE THAT IS CHARGE THAT IS SQUARED THAT'Y WHY TOOK SQUARE ROOT TO BE AGAIN ON NORMAL DOLLAR VALUE.)
OF RESULT AND THAT IS CALLED RMSE(ROOT MEAN SQUARED ERROR) WHAT IT DO?IT TELLS ON AVARAGE HOW FAR EACH POINT FROM THE LINE.

def rmse(traget,predected):
   return(np.sqrt(np.mean(np.square(target-predected))))

REMBER THE OUTPUT OF THIS WILL GIVE U ON AVERAGE EACH ELEMENT IN THE PREDICTION DIFFERE FROM ACTUAL TARGET $8601 ITS EXAMPLE THIS MEANS IM OFF BY 8601 IT CALLED THE LOSS THE LOWER THE LOSS THE BETTER THE MODEL.

NOW WE NEED STRATEGY TO MODIFY THE WEIGHTS AND BIASES TO REDUCE THE LOSS AND IMPROVE FIT OF THE LINE.THERE ARE TWO TECHNIQUE ORDINARY LEAST SQUARED(OLS) AND SECOND IS STHOCASTIC GRADIENT DESCENT BETTER FOR LARGER DATASETS this is optimizers.
that's how linear regressions works.

now using scikit learn for this use scikit lrarns library
now if u want to find out models weights use this model.coef_ and for bias model.intercept_
ALWAYS REMEBER IF UR DOING THIS U CAN FIND WEIGHTS FOR EVRY COLUMN REMMBER IF ONE COLUMN IS HAVE HIGH CORRELATION WITH TARGET COLUMN THAT IT SHOULD HAVE HIGH WEIGHTS BUT SECOND COLUMNS IS NOT HIGHLY CORLEATED BUT WEIGHTS ARE HIGH THAT MEANS MODEL IS NOT GOOD WHEN CORRLATION IS NOT GOOD WEIGHTS WILL ALSO NOT HIGH THEN WE HAVE TO BRING THOSE COLUMNS INTO SCALE AND THAT TIME WE DO SCALING.

always remember when we have categories like natural order (cold flue warm hot)we can use ordinal encoding and if we have more then 2 categories we'll use one hot encoding.
in this if we want to improve the model don't forget to add features scaling techniques like standardization and normalization what it will do it scales down the wholw clolumns our values centered arounf 0 and standardeviation of 1.after doing this weights will make a lot more sennse.
VARIANCE IS SQUARE OF STANDARD DEVIATION.

WHAT WILL BE THE STEPS TO APPROACH MACHINE LEARNING ?
FIRST GETHER DATA AND FIND THE CORRLEATION BETWEEN FOR THE IMPUTS AND TARGET COLUMNS.
CHOOSE THE RIGHT MODEL SCALE THE COLUMNS AND APPLY ONE HOT ENCODING IF UR HAVING CATEGORICAL DATA.
TRAIN TEST SPLIT TRAIN THE MODEL PREDCT IT SEE LOSS AND TRY TO IMPROVE 








what is the differencr between loss and cost?
A cost function is a function that measures the average of the loss values over all the inputs or data samples. 
The difference between loss and cost functions is that the loss function is associated with a single input or data sample, while the cost function is associated with the entire data set or model

LOGISTIC REGRESSION 

IN THIS WE'RE DOING THE SAME 
WE TAKE THE LINEAR COMBINATION OR WEIGHTED SUM OF INPUT FEATURES
WE APPLIED SIGMOID FUNCTION THAT WILL GIVE RESULTS BETWEEN 0 TO 1 DERIVATIVE OF SIGMOID IS 0 TO 0.25 IT RANGES 
THIS NUMBERS REPRESNTS THE PROBABILITY OF INPUT BEING CLASSIFIED AS YES 
INSTEAD OF RMSE HERE THE CROSS ENTROY LOSS FUNCTION US USED TO EVALUATE THE RESULT.
CROSSENTROPY FORMULA=(YHAT,Y)=-(Y LOGYHAT +(1-Y)LOG(1-YHAT))

Z=X1*W1+X2*W2+X3*W3+B -------> YHAT=STANDARD DEVIATION(Z)------> (YHAT,Y)
WE CAN ALSO HAVE OPTIMIZATION TECHNIQUE LIKE GRADIENT DESCENT ETC.

WHAT IS SOLVER?
A solver is an algorithm that finds the optimal values of the parameters for a logistic regression model. 
THERE IS SOLVER IN LOGISTIC REGRESSION WHEN TRAINING THE LOGISITC REGRESSION DO THIS SOLVER='LIBLINEAR'
The choice of solver depends on the characteristics of the data and the model, such as the size, sparsity, number of classes, and type of regularization.
The default solver in sklearn.linear_model.LogisticRegression is ‘lbfgs’, which is a good choice for most cases. 

SO THE HOGHER THE WEIGHT MEANS ITS MORE IMPORTANT FOR PREIDCTION.AND THE SIGNS LIKE + OR - WOULD BE SHOW ITS POSITVELY CORRLEATED OR NEGETAVILY
IN THIS U CAN ALSO HAVE FIND THE PROBABILTY INSTEAD OF PREDICT FOR THIS WE CAN USE PREIDCT_PROBA THIS ONLY IN LOGISTIC REGRESSION IT WILL GIVE TWO OUTPUT PROBAILITY OF YES AND NO.


WHAT IS CONFUSION MATRIX

A confusion matrix is a table used in machine learning to evaluate the performance of a classification model. It shows the number of correct and incorrect predictions made by the model, categorized by each class. The matrix is composed of four elements:

True Positives (TP): Correctly predicted positive observations.
True Negatives (TN): Correctly predicted negative observations.
False Positives (FP): Incorrectly predicted positive observations (Type I error).
False Negatives (FN): Incorrectly predicted negative observations (Type II error).

Actual \ 
Predicted	Positive	    Negative
Positive	TP	              FN(TYPE 1 ERROR)
Negative	FP(TYPE 2 ERROR)   TN

HOW TO SEE?IF MY MODEL PREDICTED POSITIVE OR YES AND ACTUAL VALUE IS POSITIVE THAT MEANS TRUE POSITIVE MEANT IT CORRECTLY PREIDCTED POSITVE.
AND WHEN MY MODEL PREIDICT NEGATIVE VALUE AND ACTUAL VALUE IS NEGATIVE THAT MEANS ITS TRUE NEAGTIVE THAT MEANS IT CORRECTLY PRRDICTED NEGATIVE.
NOW THERE ARE TWO TYPES OF ERROR SCENARIOS TYPE 1 ERROR AND TYPE 2 ERROR.
SO IF MY MODEL PREDICTED POSITVE BUT ACTUALTARGET VALUE IS NEGATIVE THAT MEANS ITS FALSE POSITIVE MEANS TYPE 1 ERROR.
AND IF MY MODEL PREDICTED NEGATIVE BUT MY TARGET VLUE IS POSITIVE THAT MEANS FALSE NEGATIVE TYPE 2 ERROR.

IMPORTANT THING U HAVE TO AVOID FALSE NEGATIVE IF I HAVE HIGH NUMBER OF FALSE NEGATIVE IN THAT IM NOT INTRESTED IN ACCURACY IF IM DOING FORCASTING IN THAT CASE AVOID TYPE 2 ERROR TRY TO REDUCE THE TYPE 2 ERROR.
SUPPOSE IM HAVING DIFFERENT KIND OF MODEL THAT PREDICTING WHEATHER DO U HAVE BREAST CANCER OR NOT IN THAT POINT U HAVE TO TAKE CLOSER LOOK FALSE POSITIVE

SO IF U CREATE CONFUSION MATRIX WITH SKELARN.METRICS IMPORT CONFUSION_MATRIX(TARGET,PREDICTION,NORMALIE='TRUE')

AND AFTER THIS IT WILL GIVE U PROBABILITY FOR TRUE POSITIVE AND TRUE NEGATIVE  IF PROBABILITY IS 94 FOR TRUE POSITIVE THAT MEANS 94 PERCENT  CASES IT PREDICTED YES THAT MEANS IT GOOD.
AMONG THE CASES IF MY MODEL PREDICTED 47 FALSE NEGATIVE MEANS ACTUAL VALUE IS YES BUY MY MODEL PREDICTED IS NEAGATIVE THAT MEANS IT PREETY BAD I CAN'T USE IT INN FOR CASTING.  


HOW WOULD U REDUCE THE LOSS?
ADJUST WEIGHTS AND BIASES.

WHAT IS RAMDOM STATE?
WE USE RANDOM STATE FOR EACH TIME OF I RUN THE CODE I WILL GET THE SAME KIND OF RANDOMIZATION HNCE THE SAME KIND OF OUTPUT SO WE CAN REPLICATE MY RESULTS.

#DECSION TREEE
A DECSION TREE IN GENERAL PARLANCE REPRESENTS A HIRARHICAL SERIES OF BINARY DECISIONS.

LIKE IF SALRY IS BETWEEN 50K TO 80K IF SALARY IS BETWEEN THEN ILL ACCEPT THE OFFER IF ITS NOT THEN I DECLINED IT.
THEN IF SALARY IS BETWEEN 50K TO 80K THEN I CHEKED IF OFFICE IS NEAR TO ME IF YES THEN ACCEPTS THE OFFER OTHER WISE NOT.
THEN IF OFFICE IS NEAR TO ME THEN I CHECK IT PROVIDING CAB FACILATIY OR NOT IF YES THEN I ACCEPT THE OFFER.
LIKE THIS ITS WORK

WHAT IS OVERFIITING?
OFFERFIITING MEANS IF MY ACCURACY IS 99 PERCNET BUT WHEN I USE MY MODEL ON UNSCENE DATA ITS GIVE ME 78 PERCENT SCORE THAT MEANS MY MODEL IS OVERFITTED THAT MEANS IT LEARDED TRAINING EXAMPLE PERFECTLY
ITS COMMN IN TREE BASED ALGORTTHM. 
MEAN MY MODEL MEMORIZED OR LRARNED SPECIFIC TRAINING EXAMPLES AND DOES NOT GENERALIZE WELL TO EXAMPLES THAT IT HAS NOT SEEN BEFORE.

WHAT IS REGULARIZATION?
REDUCEING OVERFITTING IS KNOW AS REGULARIZATION.

HOW TO REDUCE THE OVERFITTING?

For Machine Learning:
Increase Training Data: More data can help the model generalize better1.
Reduce Model Complexity: Simplify the model to prevent it from learning the noise in the training data2.
Cross-validation: Use cross-validation techniques to ensure that the model performs well on unseen data1.
Regularization: Apply regularization methods like L1 (Lasso) and L2 (Ridge) to penalize large weights2.
Feature Selection: Choose relevant features and eliminate redundant ones to reduce the model’s complexity2.
Early Stopping: Stop training as soon as the performance on a validation set starts to degrade2.

For Deep Learning:
Data Augmentation: Generate new training samples by altering the existing ones to increase the diversity of the data3.
Dropout: Randomly drop units from the neural network during training to prevent co-adaptation of features2.
Weight Regularization: Add a penalty for weight size to the loss function, such as L1 or L2 regularization3.
Batch Normalization: Normalize the inputs of each layer to stabilize the learning process and reduce overfitting3.
Reduce Network Size: Decrease the number of layers or the number of neurons to simplify the model4.
Early Stopping: Monitor the validation loss and stop training when it begins to increase, indicating overfitting2.



HOW TO REDUCE IT?
WE CAN ALSO OUR DESCION TREE U HAVE TO IMPORT FROM SKLEARN.TREE IMPORT PLOT_TREE ,EXPORT_TREE
PLOT_TREE(MODEL,FEATURES_NAMES=X_TRAIN.COLUMNS,MAX_DEPTH=2,FILLED=TRUE)

IF WE HAVE NON-LINEAR RELATION SHIPS ITS ALWAYS TO USE DESCION TREES WHEN ITS LINEAR TRY LOGIDTIC REGRESION.

HOW DECISION TREE IS CREATED ?
SO IF U PLOT THIS DESCIOSN TREE THERE U CAN SEE GINI VALUE IN EACH BOX THIS GINI VALUE IS NOTHING BUT LOSS VALUE THIS HELPS DECIDE WHICH COLUMN SHOULD BE USED FOR SPLITTING THE DATA AND WHAT POINT COLUMN SHOULD BE SPLIT A LOWER
GINI INDICATES A BETTER SPLIT A PERFECT CLASS SPLIT (ONLY ONE CLASS EACH SIDE) HAS GINI INDEX OF 0.
INSHORT GINI SCORE DECIDES HOW GOOD IS PLOT A LOWER GINI SCORE IS MEAN GOOD SPLIT.
WHAT IS DO ACTUALLY IT LOOKED AT ALL THE DIFFERENT COLUMN AND THEN FOR EACH COLUMN IT LOOKED AT ALL THE ALL THE POSSIBLE SPLIT POINTS SO IT HAS BASSICALLY SORTED ALL THE VALUES IN THOSE COLUMNS IN INCREASING ORDER AND THEN IT HAS TAKEN EACH VALUE AS SPLIT POINT.
AND THEN FOR EACH SPLIT POINT IT PERFORMED A SPLIT.AND BASED ON THE SPLIT IT HAS CALCULATED THE GINI SCORE.
SO IF I HAVE BOX LESS THEN HUMIDITY<=0.7 IS THE MOST IMPORTANT FACTOR IT LRADS TO LOWEST GINI SCORE SO THAT'S HOW IT FIGURED IT OUT WHAT SHOULD BE THE TOP ROOT LEVEL NODE.WHICH IS BEST SPLIT AMONG ALL THE COLUMNS AND ALL THE POSSIBLE SPLITS


OVERVIEW=====
SO WHAT WE DO FIRST IN INPUT WE HAVE ALL THE TRAINIG DATA AND WE ESSENATIALLY LOOK AT ALL POSSIBLE SPLIT AND WE TAKE ALL THOSE POSSIBLE SPLIT AND THEN WE COMPUTE TH GINI SCORE FOR ALL THE POSSIBLE SPLITS ACROSS ALL THE POSSIBLE COLUMNSN BAESD ON THE GINI SCORE WE PICKED THE BEST POSSIBLE SPLIT THEN WE SPLIT THE DATA INTO BASED ON THE SPLIT WAS DECIDED AND THEN WE REPEAT THE SCORE RECUSROVILY FOR EACH SPLIT FOR THE LEFT SPLIT AND RIGHT SPLIT. 
NOW HOW LONG ITS GONA RUN UNTILL WE HAVE SINGLE VALUE.IN 
BOX U CAN SEE IN SAMPLE WE HAVE TOTAL SAMPLE OR TRAINING DATA AND IN VALUE U CAN SEE 76K ROWS TO LEFT RIGHT SIDE 22K ROWS TO RIGHT.

THAT'S ROUGHLY HOW ITS WORK IT DIVIDE INTO MULTIPLE PARTS AND KEEP DOING IT TILL IT GETS TO SINGLR LEAF NODE.AND FOR THAT ROW OF DATA SINCE WE ALREADY HAVE THE TARGET SO THE TARGET FOR THE THE ROW USED AS VALUE OF THE LEAF SO EVRY LEAF ULTIMATELY CONTAIN JUST ONE SAMPLE AND THAT SAMPLE ALREDAY HAD SAMPLE YES OR NO.
DESCSION TREE DOSENT GO LAYER BY LAYER

HYPERPARAMETERS FOR DESCSION TEREE
MAX_DEPTH WHEN UR MODEL IS OVER FITTED TRY THIS TO REDUCE THE OVERFITTING
MODEL.TREE_.MAX_DEPTH#THIS IS USED TO FIND THE OVERALL DEPTH OF TREE IF MY MODEL GO TOO DEEP SO IT WILL LEARN ALL THE EXAMPLE SO THAT'S WHY DON'T GO TOO DEEP

LEAF_NODE IN LEAF NODE THE DCSION TREE DOSENT GO LAYER BY LAYER IT COMPARE ALL THE LEAF AND IT FEGURE OUT BEST LEAF NODE TO SPLIT AT THAT MOMENT.
 

#RANDOM FORREST

RANDOM FORREST IS NOTHIN BUT COMBINATION OF RESULTS OF DESCION TREES WITH DIFFERENT DIFFERENT HYPER PARAMETERS.

THE KEY IDEA HERE IS THAT EACH DESCSION TREE IN THE FORREST WILL MAKE DIFFERENT DIFFERENT KIND OF ERRORS AND UPON AVERAGING MANY OF THERE ERROS WILL CANCEL OUT.

THE RANDOM FORREST WORKS BY AVERAGING COMBINING RESULTS OF THE SEVERAL DESCION TREES 

NOW IF WE'RE DOING CLASSIFICATION THE WAY WE COMBINE THE RESULTS BY VOTING WHICH IS PICKING MOST COMMON CLASS IF ITS REGRESSION SIMPLY DO AVERAGE.

MAIN THING IN RANDOM FORREST IS IT PROVIDES RANDOMENSS AND AVRAGING OUT OF ERRORS TO CANCEL OUT ERORS. 
WHAT IS ENSEMBLING?
COMBINING  RESULTS OF MANY MODEL IS CALLED ENSEMBLING.

HOW TO KNOW HOW MANY DESCION TREES ARE THERE?
LEN(MODEL.ESTIMATORS_[0])

HYPERPARAMETER TUNING FOR RANDOM FORREST
N_ESTIMATORS IT CONTROLS THE NUMBER OF DESCION TREES IN RANDOM FORREST THE DEFAULT VALUE IS HUNDREAD FOR LARGER DATASET
IN RAMDOM FORREST U CAN ALSO APPLY MAX_DEPTH AND MAX_LEAF_NODE THIS WILL APPLY ON EVRY DECSION TREE DEFAULT VALUE IS ZERO U HAVE TO APPLY IT FOR BOTH 
NOW WE HAVE MAX_FEATURES USING THAT OUR MODEL CHOOSE FEATURES RANDOMLY FOR FIRST SPLIT IT WILL CHOSSE FRACTION OF COLUMN WHY WE'RE USING IT?
IN DESCION TREE WE CHECK EVRY COLUMN TO FIND BEST SPLIT FOR EVRY COLUMN ILL CHECK BEST POSSIBLE SPLIT THAT'S HOW U DECIDE  NOW TROUBLE WITH RANDOM FORREST IS IF IM HAVING TWO DESCSION TREES AND UR USING EXACT SAME LOGIC OF CHECING ALL COLUMNS AND FINDING BEST SPLIT
SO BOTH SPLIT GOING TO SAME THEN THERE NEXT SPLIT WILL BE THE SAME SO IM NOT TRAINING MULTIPLE DESCION TREES IM TRAINING SAME COPY OF DESCIONS TREE.SO IM NOT GAINING RANDOMNESS AND  AVRAGING OUT OF ERRORS. THAT'S WHY WE USE MAX_FEATURES. 
bOOTSTRAPING THIS WILL PICK RANDOMLY ANY ROW SUPPOSE IM HAVING 10 ROWS IT WILL PICK ANY ROWS IN ANY ORDER USING THIS WE'LL TRAIN OUR DESCION TREE SO DIFFERENCE BOOTSTRAP DROP DIFFERENT ROWS AND IT WILL TRAIN ONLY FRACTION OF DATA RATHER THEN WHOLE DATA.
now im havimg class weights suppose my target have 77 percent of no and 22 percent of yes so my class is imbalaneced using class weights in random forrest we can give class weights so we can proper class weights for each one like for no:1,and yes:2 or u can also use 'balanced'  


#GRADIENT BOOSTING
HOW ITS WORK ?
IT TAKES AVERAGE VALUE OF THE TARGET COLUMN AND USES AS INITIAL PREDICTION FOR EVERY INPUT.
THE RESIDUALS OR SAY LOSS OF PREDICTION WITH TARGET ARE COMPUTED.
AFTER DOING THIS WE HAVE RESIDUAL FROM TARGET AND INITIAL PREDICTION AND NOW ON MY DESCSION TREE WILL TRAIN ON THE BASIS ON THIS RESUDUAL NOT ON ACTUAL TARGET VALUE.
AFTER THIS WE'LL HAVE OUR PREDICTION FROM 1ST DESCION TREE SO IF U ADD THE PREDICTED VALUE FROM ACTUAL TARGET VALUE U CAN SEE THEY ARE CLOSE THEN THE AVERAGE VALUE THAT WE TAKE AS INITAL PREDICTION.
SO ALMOST ALL THE PREDICTION NOW BECOME BETTER BECAUSE WE HAVE TRAINED A DESION TREE TO COMPUTE RESIDUALS AS SOON WE AS WE CREATE ONE DESION TREE OUR PREDICTION BECOME BETTER AND HOW WE MAKE THE PREDICTION?
WE SIMPLY ADD THE ORIGINAL PREDICTION + THE PREDICTION THAT OUR 1ST DECSION TREE RESIDUAL BY DESION TREE.
SO STILL WE DON'T HAVE PERFECT MODEL SOO WE TRAIN ANOTHER DESCION TREE TO COMPUTE THE RESIDUAL THAT WERE OBTAINED AFTER TAKING THE RESULTS FROM ORGINAL GUESS AND FIRST DESCION TREE.SO THE SECOND DESCION TREE IS TRYING TO CORRECT THE ERRORS MADE BY THE SECOND DESCION TREE AND THAT REDUCE ERROR FURTHER.
AND THEN WE TRAIN ANOTHER DESCION TREE AND THAT ATTMPTS TO CORRECT THE  ERROS MADE BY THIRD DESCION TREE. SO THAT'S HOW GRADIENT BOOSTING WORK.


THE TERM GRADIENT REFERS TO THE FACT THAT EACH DESION TREE IS TRAINED WITH THE PURPOSE OF REDUCING THE LOSS FROM THE PREVIOUS ITERATION 

SUCCESSIVELY IMPROVING OUR PREDICTION BY TRAINING THESE SMALL DESICION TREES TO CONNECT THE ERRORS OF THE MODEL THAT WE HAVE SO FAR.

WHY WE'RE USING LEARNING RATE IN GRADIENT BOOSTING WHY NOT IN RANDOM FORREST AND DESCSION TREE?
WHY WE JUST DON'T ADD THE PREDICTION FROM PREVIOUSLY TREE CUZ ALPHA MEANS LEARNING RATE PREVENTS FROM OVERFITTING IF WE APPLY THE SCALING FACTOR TO DESCION TREE THAT WAS TRAINED TO PREDICT RESIDUALS(ERROS) SO WE ARE INTENTIONALLY CREATING SOME ERROR THERE WHICH CAN BE THEN FILLED IN BY THE NEXT 
DESCION TREE AND SO ON AND THAT HELPS PREVENT OVERFITTING TO THE TRAINING SET.

OVERVIEW 
WE GIVE INPUTS TO THE MODEL IT MAKES THE FIRST GUEST THAT IS AVERAGE WE COMPUTE THE LOSS WE COMPUTE RESIDUALS AND THEN WE TRAIN DESCION TREE TO PREDICT THE RESIDUALS.
NOW WE PUT INPUTS ONCE AGAIN NOW WE GET OUTPUT AS AVERGE PREDICTION PLUS THE PREDICTION OF DESCION TREE SCLAED DOWN THEN WE PREDICT THE NEXT SET OF RESIDUALS AGAIN USING THE TARGETS AND THEN WE OPTIMIZE TRAINING ANOTHER DESCION TREE.

Xgboost also have frature importance we this u can have only features that are important and it also prevent overfitting with model.feature_importance_


what is cross validation?
cross validation is technoque used to evaluate the performance of a model on unseen data.
it involves dividing the availabe data into multiple folds or subsets.one of this folds serve as validation set while the model trains on the remaining folds.
Finally, the results from each validation step are averaged to produce a more robust estimate of the model’s performance.

Why Use Cross-Validation?
The main purpose of cross-validation is to prevent overfitting.
Overfitting occurs when a model performs well on the training data but poorly on new, unseen data.
By evaluating the model on multiple validation sets, cross-validation provides a more realistic estimate of the model’s generalization performance.

hyperparameters
n_estimators this is used to control the number of descion tree.
its also have max_depth is simply depth of each tree will have the trees we created to correct the residuals what is the maximum depth i want to allow them to have.initally if u increased it 
the loss will go down. if go too deep our model will be overfitted.
learning_rate 
booster in this we can use linear model to compute the residual.and then we agai train linear model to compute the correct residuals.


what is learning rate?
The learning rate (denoted by α) is a hyperparameter that controls the step size at which an algorithm updates the weights or parameters of a model during training.
role of learning rate 
During training, the learning rate determines how quickly the model converges to an optimal solution.
Too High: If the learning rate is too high, the model may overshoot the optimal weights and fail to converge.
Too Low: If the learning rate is too low, the model may take too long to converge or get stuck in local minima.
Grid Search or Random Search: Systematically try different learning rates and evaluate their impact on model performance.
Learning Rate Schedules: Use adaptive learning rates that change during training (e.g., learning rate decay, momentum, or Adam optimizer)
examples
Imagine learning to play a video game where timing matters (e.g., jumping over obstacles).
If you adjust your timing too much (large learning rate), you might overshoot or fail.
But small adjustments (moderate learning rate) can lead to consistent success.


why we're using learning rate in gradient boosting not in randomforrest ?xgboost using lot of descion tree 
what's the role of learning rate?
Gradient Boosting Trees (GBT):
In gradient boosting, the learning rate (also called shrinkage or eta) controls how quickly the model learns from the mistakes of previous trees.
Sequential Learning:
GBT builds an ensemble of decision trees sequentially.
Each new tree corrects the errors made by the previous ones.
The learning rate determines how much each tree contributes to the final prediction.
Random foreest:
Random forests do not use a learning rate.Each tree in a random forest is built independently.There is no sequential correction of errors.
Parallel Training:
Random forests train multiple decision trees in parallel.
Each tree is trained on a random subset of data and features.
The final prediction is an average (regression) or majority vote (classification) of individual tree predictions.



#UNSUPERVISED LEARNING 
REMMBER IF U JUST WANT TO SEE THE OR VISUALIZE THE DATA OR REDUCE THE DIMENSIONS OR SAY ITS SIZE THEN WE DO PCA(PRINCIPAL COMPONENT ANALYSIS ) DIMENSIONALITY REDUCTION.
IF I WANT TO PREDICT THE QUANTOITY THEN I DO REGRESION.


WHAT IS CLUSTERING?
CLUSTERING IS THE PROCESS OF GROUPING OBJECTS FROM THE DATASET THAT ARE SIMILAR IN SOME SENSE.
MAIN PURPOSE IS TO FIND OUT HOW MANY CLUSTERS ARE THERE  AND WHICH CLUSTERS TO EACH POINTS BELONGS TO.

REAL WORLD APPLICATIONS
CUSTOMER SEGMENTATIONS
PRODUCT RECOMENDATIONS IF U CAN IDENTIFY CLUSTERS OF PEOPLE WHO LIKES PERTICULAR PRODUCT THEN U CAN RECOMED THE SAME.
FEATURE ENGINNERING 
ANONLY FRAUD DETECTION
TAXONMY CREATION

K-MEANS CLUSTERING
THE K MEANS CLUSRERING ATTEMPTS TO CLASSIFY OBJECTS INTO PRE -DETERMIND NUMBER OF CLUSTERS BY FINDING OPTIMAL CENTRAL POINT CALLED CENTROIDS FOR EACH CLUSTERS.

HOW ITS WORKS?
IT PICKS K RANDOM OBJECTS AS INITIAL CLUSTERS CENTERS. THAT MEANS ILL PICK RANDOM K=3 MEANS THIS WILL MY CENTER POINTS RANDOMLY IN DATASET.
CLASSIFY EACH OBJECT INTO CLUSTERS WHOSE CENTER IS CLOSEST TO THE POINT.
FOR EACH CLUSTER OF CLASSIFIED OBJECTS  COMPUTE THE CENTROID MEAN AFTER THAT THIS WILL BECOME NEW CSNTER OF CLUSTER.
NOW RECLASSIFY EACH OBJECT USING THE CENTROID AS CLUSTER CENTER. 
CALCULATE TOTAL VARIANCE OF THE CLUSTERS (THIS IS THE MEASSURE OF GOODNESS).
REPEAT THE STEPS 1 TO 6 A FEW MORE TIMES AND PICK THE CLUSTERS CENTER WITH THE LOWEST TOTAL VARIANCE.

SO SOMETIME SUPPOSE WE A POINT WE PICKED OUT IN OUR RANDOM SELECTION INITIALLY WERE VERY BAD POINTS(cluster center).WHAT WOULD HAPPEN THEN?THEN MAYBE
ONLY ONE POINT HAVE ALL THE CLUSTERS DATA AND OTHER TWO HAVING LESS. SO K MEANS IS DOES NOT WORK PERFECTLY IT DEPENDS ON THE RANDOM PICK SO WE PICK 4 TO 5 TIMES RANDOMLY THEN WE COMPUTE 
THE COMPUTE CENTROID(MEAN) FOR EACH SET OF CLUSTER.AFTER DOING THIS 14 OR 15 TIMES IN THIS WE SIMPLY PICK THE ONE WHERE WE GOT THE BEST LOWEST TOTAL VARIANCE.
sklearn.clsuter import KMeans
model=KMeans(n_clusters=3,random_state=42)
model.fit(x)
we can check the clsuters centers for each cluster 
model.cluster_centers_
now we do model.predict(x)

now the qustion is how to determine the distance between point and the data points so i can now which data poitns belongs to which point.
so we do this euclidain distance.

we check the goodness of the fit by looking model.inertia_ which contains the sum of squared distances of samples to theier closet cluster centerlower the inertia means 
better fit.


#NOW DOING DBSCAN
DBSCAN DENSITY BASED SPATIAL CLUSTERING OF APPLICATION WITH NOISE USES THE DESNSITY OF POINTS IN A REGION TO FORM CLUSTERS
IT HAS TWO MAIN PARAMETERS EPSILON AND MINISAMPLE  USING WHICH IT CLASSIFY EACH POINT AS CORE POINT 

SO HERE HOW ITS WORK SUPPOSE IN Y AND X ASIS WE'RE HAVING OUR DATA POINTS
FIRST WE HAVE OUR EPSILON=0.5 AND MIN_SAMPLE=4 WE SET THIS AFTER WE CAN SART WITH ANY POINT AND IT DRAW CIRCLE WITH RADIUS EPSILON.AND THEN WE CHECK 
AT LEAST WE HAVE 4 POINTS SAMPLETHEN WE SAY ITS CORE POINT AND EVRYTHING THAT CONNECT TO THE CORE POINT IS NOW PART OF THE SAME CLUSTER.THEN WE GO TO ANOTHER POINT 
AND DRAW THE CIRCLE OF SIZE EPSILON AND NOW WE CHECK ITS HAVING AT EAST 4 POINTS LIKE THIS WILL KEPP DOING THIS CONNECTING CORE POINTS SOME POINTS WILL NOT CONNECT THIS WILL PART OF 
CLUSTER BUT NOT THE CORE POINT. IF A POINT IS NOR A CORE POINT OR NOT IN EPSILON RADIUS WE CALLED OUTLIER OR NOISE.

WHEN TO USE K MEANS OR DBSCAN?
IN K MEANS WE DEFINE HOW MANY CLUSTERS DO U WANT.
BUT IN DBSCAN IT WILL FIGURE OUT ITS OWN.AND U CAN ONLY CHANGE THE EPSILON
IF I WANT TO DICOVER PATTERNS THAT ARE MORE CONCERN WITH NEARNESS OF POINT THEMSELF DBSCAN WILL MAKE MORE SCENCE.
IF I DISTANCE BASED CLUSTERING THEN I USE K-MEANS

#DIMENTIONALITY REDUCTION OR MANIFLOD TECHNIQUES
WE USE PCA(PRINCIPAL COMPONENT ANALYSIS) IF IM HAVING HIGHER DIMESNIONS OR THOUSAND COLUMNS OR DOZEN COLUMNS SO WHAT IF WE COULD REDUCE THE NUMBER OF DIMENSION 
WITHPUT LOSSING LOTS OF DATA
APPLICATION
REDUCING SIZE OF DATA WITHOUT LOSS OF INFROMATION 
TRAINING MACHINE LEARNING MODEL EFFICIENTLY
VISUALIZING High DIMENSIONS DATA IN 2/3 DIMENSIONS

